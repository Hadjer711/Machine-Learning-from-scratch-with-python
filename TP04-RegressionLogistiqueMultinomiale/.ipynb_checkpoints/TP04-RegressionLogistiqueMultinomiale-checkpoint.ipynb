{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 04 : Régression logistique Multinomiale\n",
    "\n",
    "Nous avons implémenté le cas d'une seule classe (binaire : oui ou non). Pour appliquer un classement sur plusieurs classes, on peut entrainner $L$ modèles de régression logistique (où $L$ est le nombre des classes). Dans ce cas, nos résultats (Y) doivent encodée en 0 et 1. Pour un modèle $M_i$ d'une classe $C_i$, la sortie $Y$ doit avoir 1 si $C_i$, 0 si une autre classe. (One-to-rest classification)\n",
    "\n",
    "Une autre approche (celle qu'on va implémenter) est d'encoder la sortie en utilisant OneHot encoder. Pour $L$ classes et un échantillon donnée, on va avoir $L$ sorties (une ayant 1 et les autres 0). Pour un dataset avec $M$ échantillons, $N$ caractéristiques et $L$ classes, on va avoir les dimensions suivantes : \n",
    "- $X (M, N)$\n",
    "- $Y (M, L)$\n",
    "- $\\theta (L, N)$\n",
    "\n",
    "Cette dernière approche s'appelle maximum entropy (MaxEnt). Elle généralise la régresion logistique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Implémentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1- Fonctions utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Défininir des fonctions qui génèrent des vecteurs ou des matrices\n",
    "# Ces fonctions seront utilisées pour générer les Thétas\n",
    "\n",
    "def generer_zeros_2(nbr_x, nbr_y):\n",
    "    return np.zeros((nbr_x, nbr_y))\n",
    "\n",
    "def generer_uns_2(nbr_x, nbr_y):\n",
    "    return np.ones((nbr_x, nbr_y))\n",
    "\n",
    "def generer_aleatoire_2(nbr_x, nbr_y):\n",
    "    return np.random.rand(nbr_x, nbr_y)\n",
    "    \n",
    "generer_zeros_2(2, 3), generer_uns_2(2, 3), generer_aleatoire_2(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tn = np.array([[0., 0.], [1., 0.], [0., 1.], [1., 1.]])\n",
    "Y_tn = np.array([[1,0,0], [0,1,0], [0,0,1], [1,0,0]])\n",
    "Theta_tn = np.array([[0.5, 0.2], [0.1, 0.3], [0.6, 0.]])\n",
    "\n",
    "def preparer(X, norm=True, const=True, mean=None, std=None): \n",
    "    X_pre = X.copy()\n",
    "    if norm: \n",
    "        X_pre, mean, std = normaliser(X_pre)\n",
    "    if const:\n",
    "        X_pre = np.append(np.ones((X_pre.shape[0],1)), X_pre ,axis=1)\n",
    "    return X_pre, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tn = np.array([[0., 0.], [1., 0.], [0., 1.], [1., 1.]])\n",
    "Y_tn = np.array([[1,0,0], [0,1,0], [0,0,1], [1,0,0]])\n",
    "Theta_tn = np.array([[0.5, 0.2], [0.1, 0.3], [0.6, 0.]])\n",
    "\n",
    "def normaliser(X, mean=None, std=None): \n",
    "    if (mean is None) or (std is None): \n",
    "        mean = np.mean(X, axis=0)\n",
    "        std = np.std(X, axis=0)\n",
    "    X_norm = (X - mean)/std\n",
    "    return X_norm, mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2- Combinaison linéaire\n",
    "On combine les m caractéristiques linéairement. La seule chose différente est que z dans ce cas est une matrice. (nombre_échantillons * nombre_classes) \n",
    "\n",
    "$$z=\\theta_0+\\sum\\limits_{i=1}^{m} \\theta_i x_i = X \\theta^T $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO implémenter la fonction de combinaison linéaire \n",
    "def z_n(X, Theta): \n",
    "    return None\n",
    "\n",
    "X_tn = np.array([[0., 0.], [1., 0.], [0., 1.], [1., 1.]])\n",
    "Y_tn = np.array([[1,0,0], [0,1,0], [0,0,1], [1,0,0]])\n",
    "Theta_tn = np.array([[0.5, 0.2], [0.1, 0.3], [0.6, 0.]])\n",
    "\n",
    "#Résulat : \n",
    "# array([[0. , 0. , 0. ],\n",
    "#       [0.5, 0.1, 0.6],\n",
    "#       [0.2, 0.3, 0. ],\n",
    "#       [0.7, 0.4, 0.6]])\n",
    "z_n(X_tn, Theta_tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2- Calcul des probabilités\n",
    "\n",
    "Les valeurs combinées sont transformées à des probabilités en utilisant la fonction softmax. $L$ est le nombre de classes. $Z$ est un vecteur des comninations linéaires.\n",
    "\n",
    "Dans la fonction softmax génère des poids où la somme des colonnes toujours donne 1.0 \n",
    "\n",
    "$$softmax(Z)=\\frac{e^Z}{\\sum\\limits_{k=1}^{L} e^{Z_k}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO compléter la fonction softmax\n",
    "def softmax(X):\n",
    "    return None\n",
    "\n",
    "X_tn = np.array([[0., 0.], [1., 0.], [0., 1.], [1., 1.]])\n",
    "Y_tn = np.array([[1,0,0], [0,1,0], [0,0,1], [1,0,0]])\n",
    "Theta_tn = np.array([[0.5, 0.2], [0.1, 0.3], [0.6, 0.]])\n",
    "\n",
    "# Résultat : \n",
    "# array([[0.33333333, 0.33333333, 0.33333333],\n",
    "#       [0.36029662, 0.24151404, 0.39818934],\n",
    "#       [0.34200877, 0.37797814, 0.28001309],\n",
    "#       [0.37797814, 0.28001309, 0.34200877]])\n",
    "softmax(z_n(X_tn, Theta_tn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_n(X, Theta_n, mean=None, std=None, const=False): \n",
    "    norm = (mean is not None) and (std is not None)\n",
    "    X_pre, mean, std = preparer(X, norm, const, mean=mean, std=std)\n",
    "    return softmax(z_n(X_pre, Theta_n))\n",
    "\n",
    "X_tn = np.array([[0., 0.], [1., 0.], [0., 1.], [1., 1.]])\n",
    "Y_tn = np.array([[1,0,0], [0,1,0], [0,0,1], [1,0,0]])\n",
    "Theta_tn = np.array([[0.5, 0.2], [0.1, 0.3], [0.6, 0.]])\n",
    "\n",
    "# Résulat : \n",
    "# array([[0.33333333, 0.33333333, 0.33333333],\n",
    "#       [0.36029662, 0.24151404, 0.39818934],\n",
    "#       [0.34200877, 0.37797814, 0.28001309],\n",
    "#       [0.37797814, 0.28001309, 0.34200877]])\n",
    "h_n(X_tn, Theta_tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3- La fonction du cout \n",
    "\n",
    "- L: nombre des classes \n",
    "- M: nombre des échantillons\n",
    "\n",
    "$$ cout(h_\\theta(x), y) = - \\sum\\limits_{c=1}^{L} y_c \\log(h_{\\theta,c}(x))$$\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{M} \\sum\\limits_{i=1}^{M} cout(h_\\theta(x^{(i)}), y^{(i)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO compléter \n",
    "def J_n(H, Y): \n",
    "    return None\n",
    "\n",
    "X_tn = np.array([[0., 0.], [1., 0.], [0., 1.], [1., 1.]])\n",
    "Y_tn = np.array([[1,0,0], [0,1,0], [0,0,1], [1,0,0]])\n",
    "Theta_tn = np.array([[0.5, 0.2], [0.1, 0.3], [0.6, 0.]])\n",
    "\n",
    "# 1.1913194426181715\n",
    "J_n(h_n(X_tn, Theta_tn), Y_tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4- Les gradients\n",
    "\n",
    "$$\\frac{\\partial J(\\theta)}{\\theta_j} = \\frac{1}{M} \\sum\\limits_{i=1}^{M} (h_\\theta(x^{(i)}) - y^{(i)}) x_{ij} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO compléter\n",
    "def gradient_n(X, H, Y):\n",
    "    return  None\n",
    "\n",
    "X_tn = np.array([[0., 0.], [1., 0.], [0., 1.], [1., 1.]])\n",
    "Y_tn = np.array([[1,0,0], [0,1,0], [0,0,1], [1,0,0]])\n",
    "Theta_tn = np.array([[0.5, 0.2], [0.1, 0.3], [0.6, 0.]])\n",
    "\n",
    "# Résultat :\n",
    "# array([[-0.06543131, -0.07000327],\n",
    "#       [-0.11961822,  0.16449781],\n",
    "#       [ 0.18504953, -0.09449454]])\n",
    "gradient_n(X_tn, h_n(X_tn, Theta_tn), Y_tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5- Entraînnement (algorithme du gradient)\n",
    "\n",
    "Les coéfficients sont mis à jour itérativement en se basant sur le gradient et un pas d'apprentissage $\\alpha$. Puisque cette fonction a été implémentée dans le TP précédent, elle est donnée ici, mais d'une façon plus parametrable.\n",
    "\n",
    "$$\\theta_j = \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\theta_j} $$\n",
    "\n",
    "Notre fonction d'entrainnement prend les paramètres suivantes : \n",
    "- X : matrice (échantillons X caractéristiques)\n",
    "- Y : vecteur (ou matrice) des résultas (échantillons X nombre_classes)\n",
    "- norm : si on normalise X ou nom (par défaut : True)\n",
    "- const : si on ajoute $\\theta_0$ ou non (par défaut : True)\n",
    "- nbr_iter : nombre des itérations avant de sortir\n",
    "- alpha : le pas d'apparentissage (Learning rate)\n",
    "- eps : le test d'arrêt si la différence entre les couts (actuel et précédent) est inférieur à $\\epsilon$ on arrête la désente même si on n'a pas terminé toutes les itérations\n",
    "- theta_func : la fonction qui génère les $\\theta$ (par défaut : zéros)\n",
    "- h_func : la fonction qui calcule les probabilités (par défaut : h_1)\n",
    "- J_func : la fonction du cout (par défaut : J_1)\n",
    "- grad_func : la fonction qui calcule le gradient (par défaut : gradient_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrainer_n(X, Y, norm=True, const=True, nbr_iter=200, alpha=1., eps=0.01, \n",
    "                theta_func=generer_zeros_2, h_func=h_n, J_func=J_n, grad_func=gradient_n):\n",
    "    \n",
    "    X_pre, mean, std = preparer(X, norm=norm, const=const)\n",
    "    \n",
    "    Theta = theta_func(Y.shape[1], X_pre.shape[1])\n",
    "    \n",
    "    couts = []\n",
    "    couts.append(J_func(h_func(X_pre, Theta), Y))\n",
    "    \n",
    "    for i in range(nbr_iter):\n",
    "        H = h_func(X_pre, Theta)\n",
    "        Theta -= alpha * grad_func(X_pre, H, Y)\n",
    "        couts.append(J_func(H, Y))\n",
    "    \n",
    "    return Theta, mean, std, couts\n",
    "\n",
    "X_tn = np.array([[0., 0.], [1., 0.], [0., 1.], [1., 1.]])\n",
    "Y_tn = np.array([[1,0,0], [0,1,0], [0,0,1], [1,0,0]])\n",
    "Theta_tn = np.array([[0.5, 0.2], [0.1, 0.3], [0.6, 0.]])\n",
    "\n",
    "theta_n, mean_n, std_n, couts_n = entrainer_n(X_tn, Y_tn, const=False)\n",
    "\n",
    "# Résultat : \n",
    "# (array([[ 5.55111512e-17, -2.77555756e-17],\n",
    "#        [ 2.32810552e+00, -2.32810552e+00],\n",
    "#        [-2.32810552e+00,  2.32810552e+00]]),\n",
    "# array([0.5, 0.5]),\n",
    "# array([0.5, 0.5]),\n",
    "# 0.5541027976268512)\n",
    "theta_n, mean_n, std_n, couts_n[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6- Prédiction \n",
    "\n",
    "Etant donnée un seuil et des probabilités, pour chaque probabilité on rend 1 si elle dépasse ou égale le seuil, 0 sinon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO compléter cette fonction \n",
    "# Elle doit calculer la \n",
    "# H est un vecteur de probabilités \n",
    "def predire_n(H): \n",
    "    return None\n",
    "\n",
    "# Résultat : \n",
    "# array([[1, 1, 1], # dans cette ligne, il faut mieux rendre un seul 1 et les autres 0\n",
    "#       [0, 0, 1],\n",
    "#       [0, 1, 0],\n",
    "#       [1, 0, 0]])\n",
    "predire_n(h_n(X_tn, Theta_tn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7- Regrouper les fonctions ensemble \n",
    "\n",
    "Pour bien gérer l'entrainnement et la prédiction, on rassemble les fonctions que vous avez implémenté dans une seul classe. L'intérêt : \n",
    "- Si on applique la normalisation durant l'entrainnement, on doit l'appliquer aussi durant la prédiction. En plus, on doit utiliser les mêmes paramètres (moyenne et écart-type)\n",
    "- On utilise les thétas optimales lors de la prédicition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxEnt(object):\n",
    "    \n",
    "    def __init__(self, nbr_iter=100, alpha=1., theta_func=generer_zeros_2, norm=True, const=True): \n",
    "        self.nbr_iter = nbr_iter\n",
    "        self.alpha = alpha\n",
    "        self.theta_func = theta_func\n",
    "        self.norm = norm\n",
    "        self.const = const\n",
    "    \n",
    "    def entrainer(self, X, Y): \n",
    "        #encoder = OneHotEncoder(sparse=False)\n",
    "        #self.yencoder = encoder.fit(Y)\n",
    "        # Y = self.yencoder.transform(Y)\n",
    "        self.Theta, self.mean, self.std, self.couts = entrainer_n(X, Y, \n",
    "                                                                  nbr_iter=self.nbr_iter, \n",
    "                                                                  alpha=self.alpha, \n",
    "                                                                  theta_func=self.theta_func, \n",
    "                                                                  norm=self.norm, \n",
    "                                                                  const=self.const)\n",
    "        \n",
    "        \n",
    "    # La prédiction\n",
    "    # si prob=True elle rend un vecteur de probabilités\n",
    "    # sinon elle rend une vecteur de 1 et 0\n",
    "    def predire(self, X, prob=True, seuil=0.5):\n",
    "        H = h_n(X, self.Theta, self.mean, self.std, self.const)\n",
    "        if prob:\n",
    "            return H\n",
    "        return predire_n(H, seuil=seuil)\n",
    "\n",
    "X_tn = np.array([[0., 0.], [1., 0.], [0., 1.], [1., 1.]])\n",
    "Y_tn = np.array([[1,0,0], [0,1,0], [0,0,1], [1,0,0]])\n",
    "Theta_tn = np.array([[0.5, 0.2], [0.1, 0.3], [0.6, 0.]])\n",
    "\n",
    "# Résultat: \n",
    "# array([[9.41211100e-03, 9.90498451e-01, 8.94376295e-05],\n",
    "#       [9.41211100e-03, 8.94376295e-05, 9.90498451e-01]])\n",
    "maxent = MaxEnt(const=False)\n",
    "maxent.entrainer(X_tn, Y_tn)\n",
    "maxent.predire(np.array([[2., -2.],[-1., 1.]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Application sur un exemple réel\n",
    "\n",
    "On va utiliser [Iris dataset](https://archive.ics.uci.edu/ml/datasets/iris) pour classer des fleurs en trois classes, en utilisant 4 caractéristiques. Pour simplification, on va utiliser seulement 2 caractéristiques: Petal Length (cm); Petal Width (cm). D'après [Ce tutoriel](https://teddykoker.com/2019/06/multi-class-classification-with-logistic-regression-in-python/) ces 2 caractéristiques sont suffisantes.\n",
    "\n",
    "**Dans cette partie, vous n'avez rien à programmer. Mais, il faut analyser les résultats à la fin**\n",
    "\n",
    "Deux solutions à analyser : \n",
    "- Entrainer 3 modèles de régression logistique binaire\n",
    "- Entrainer 1 modèle de régression logistique multinomial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.read_csv(\"datasets/iris.csv\")\n",
    "iris = iris.sample(frac=1)\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if iris.shape[1] > 3:\n",
    "    iris.drop([\"sepal_length\", \"sepal_width\"], axis = 1, inplace=True)\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction des features \n",
    "X_iris = iris.iloc[:, :-1].values # Premières colonnes \n",
    "\n",
    "Y_iris = iris.iloc[:,-1].values # Dernière colonne \n",
    "\n",
    "setosa = iris[\"class\"] == \"Iris-setosa\"\n",
    "versicolor = iris[\"class\"] == \"Iris-versicolor\"\n",
    "virginica = iris[\"class\"] == \"Iris-virginica\"\n",
    "\n",
    "plt.scatter(X_iris[setosa, 0], X_iris[setosa, 1], color=\"red\", label=\"Iris-setosa\")\n",
    "plt.scatter(X_iris[versicolor, 0], X_iris[versicolor, 1], color=\"blue\", label=\"Iris-versicolor\")\n",
    "plt.scatter(X_iris[virginica, 0], X_iris[virginica, 1], color=\"green\", label=\"Iris-virginica\")\n",
    "\n",
    "plt.xlabel(\"sepal_length\")\n",
    "plt.ylabel(\"sepal_width\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomization des données pour marquer les 80% lignes\n",
    "# Le même masque sera utilisé pour les deux solutions\n",
    "iris_msk = np.random.rand(len(X_iris)) < 0.8 \n",
    "X_iris_train = np.array(X_iris[iris_msk, :], dtype=np.float32)\n",
    "X_iris_test = np.array(X_iris[~iris_msk, :], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodage des classes \n",
    "Y_iris_setosa = (Y_iris == \"Iris-setosa\").astype(int)\n",
    "Y_iris_versicolor = (Y_iris == \"Iris-versicolor\").astype(int)\n",
    "Y_iris_virginica = (Y_iris == \"Iris-virginica\").astype(int)\n",
    "\n",
    "Y_iris_setosa.sum(), Y_iris_versicolor.sum(), Y_iris_virginica.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder setosa par 1\n",
    "Y_iris_setosa_test = np.array(Y_iris_setosa[~iris_msk], dtype=np.float32)\n",
    "# encoder versicolor par 2\n",
    "Y_iris_versicolor_test = np.array(Y_iris_versicolor[~iris_msk], dtype=np.float32) * 2\n",
    "# encoder la sortie par 3\n",
    "Y_iris_virginica_test = np.array(Y_iris_virginica[~iris_msk], dtype=np.float32) * 3\n",
    "# fusionner les trois vecteurs pour avoir un seul avec les classes encodées\n",
    "Y_iris_test = Y_iris_setosa_test + Y_iris_versicolor_test + Y_iris_virginica_test \n",
    "Y_iris_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1- Entrainer 3 modèles de régression lgistique binaire\n",
    "\n",
    "Pour ce faire, recopier votre solution du TP précédent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generer_zeros_1(nbr):\n",
    "    return np.zeros(nbr)\n",
    "\n",
    "def generer_uns_1(nbr):\n",
    "    return np.ones(nbr)\n",
    "\n",
    "def generer_aleatoire_1(nbr):\n",
    "    return np.random.rand(nbr)\n",
    "\n",
    "# TODO recopier z_1(X, Theta) ici\n",
    "\n",
    "\n",
    "# TODO recopier sigmoid(X) ici\n",
    "\n",
    "\n",
    "def h_1(X, Theta, mean=None, std=None, const=False): \n",
    "    norm = (mean is not None) and (std is not None)\n",
    "    X_pre, mean, std = preparer(X, norm, const, mean=mean, std=std)\n",
    "    return sigmoid(z_1(X_pre, Theta))\n",
    "\n",
    "# TODO recopier J_1(H, Y) ici\n",
    "\n",
    "\n",
    "# TODO recopier gradient_1(X, H, Y) ici\n",
    "\n",
    "\n",
    "def entrainer_1(X, Y, norm=True, const=True, nbr_iter=200, alpha=1., eps=0.01, \n",
    "                theta_func=generer_zeros_1, h_func=h_1, J_func=J_1, grad_func=gradient_1): \n",
    "    \n",
    "    X_pre, mean, std = preparer(X, norm, const)\n",
    "    Theta = theta_func(X_pre.shape[1])\n",
    "    \n",
    "    couts = []\n",
    "    couts.append(J_func(h_func(X_pre, Theta), Y))\n",
    "    \n",
    "    for i in range(nbr_iter):\n",
    "        H = h_func(X_pre, Theta)\n",
    "        Theta -= alpha * grad_func(X_pre, H, Y)\n",
    "        couts.append(J_func(H, Y))\n",
    "    \n",
    "    return Theta, mean, std, couts\n",
    "\n",
    "# TODO recopier predire_1(H, seuil=0.5) ici\n",
    "\n",
    "\n",
    "class RegLogistique(object):\n",
    "    \n",
    "    def __init__(self, nbr_iter=100, alpha=1., theta_func=generer_zeros_1, norm=True, const=True): \n",
    "        self.nbr_iter = nbr_iter\n",
    "        self.alpha = alpha\n",
    "        self.theta_func = theta_func\n",
    "        self.norm = norm\n",
    "        self.const = const\n",
    "    \n",
    "    def entrainer(self, X, Y): \n",
    "        self.Theta, self.mean, self.std, self.couts = entrainer_1(X, Y, \n",
    "                                                                  nbr_iter=self.nbr_iter, \n",
    "                                                                  alpha=self.alpha, \n",
    "                                                                  theta_func=self.theta_func, \n",
    "                                                                  norm=self.norm, \n",
    "                                                                  const=self.const)\n",
    "        \n",
    "    # La prédiction\n",
    "    # si prob=True elle rend un vecteur de probabilités\n",
    "    # sinon elle rend une vecteur de 1 et 0\n",
    "    def predire(self, X, prob=True, seuil=0.5):\n",
    "        H = h_1(X, self.Theta, self.mean, self.std, self.const)\n",
    "        if prob:\n",
    "            return H\n",
    "        return predire_1(H, seuil=seuil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modèle 1\n",
    "Y_iris_setosa_train = np.array(Y_iris_setosa[iris_msk], dtype=np.float32)\n",
    "reg_iris_setosa = RegLogistique()\n",
    "reg_iris_setosa.entrainer(X_iris_train, Y_iris_setosa_train)\n",
    "\n",
    "#modèle 2\n",
    "Y_iris_versicolor_train = np.array(Y_iris_versicolor[iris_msk], dtype=np.float32)\n",
    "reg_iris_versicolor = RegLogistique()\n",
    "reg_iris_versicolor.entrainer(X_iris_train, Y_iris_versicolor_train)\n",
    "\n",
    "#modèle 3\n",
    "Y_iris_virginica_train = np.array(Y_iris_virginica[iris_msk], dtype=np.float32)\n",
    "reg_iris_virginica = RegLogistique()\n",
    "reg_iris_virginica.entrainer(X_iris_train, Y_iris_versicolor_train)\n",
    "\n",
    "\n",
    "#Affichage des évolutions des couts \n",
    "plt.plot(reg_iris_setosa.couts, color=\"red\", label=\"Iris-setosa\")\n",
    "plt.plot(reg_iris_versicolor.couts, color=\"blue\", label=\"Iris-versicolor\")\n",
    "plt.plot(reg_iris_virginica.couts, color=\"green\", label=\"Iris-virginica\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** : Que remarquez-vous concernant la convergence de chaque modèle?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2- Entrainer 1 modèle de régression lgistique multinomiale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodage des classes (on peut utiliser OneHotEncoder de scikit-learn)\n",
    "# Ici, on va l'implémenter en utilisant numpy pour bien comprendre comment l'encodage fonctionne \n",
    "Y_iris_onehot_train = np.column_stack((Y_iris_setosa_train, Y_iris_versicolor_train, Y_iris_virginica_train))\n",
    "\n",
    "Y_iris_onehot_train[:4, :], Y_iris[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "maxent_iris = MaxEnt()\n",
    "maxent_iris.entrainer(X_iris_train, Y_iris_onehot_train)\n",
    "\n",
    "\n",
    "#Affichage des évolutions des couts \n",
    "plt.plot(reg_iris_setosa.couts, color=\"green\", label=\"Binaire: Iris-setosa\")\n",
    "plt.plot(maxent_iris.couts, color=\"red\", label=\"Maxent: les 3 classes\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** : Que remarquez-vous? Que pouvez-vous dire à propos de la classe \"Iris-setosa\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On entraine un modèle Maxent sans le laisser converger\n",
    "# nombre des itérations 20\n",
    "maxent20_iris = MaxEnt(nbr_iter=20)\n",
    "maxent20_iris.entrainer(X_iris_train, Y_iris_onehot_train)\n",
    "\n",
    "\n",
    "#Affichage des évolutions des couts \n",
    "plt.plot(maxent20_iris.couts, color=\"red\", label=\"Maxent: 20 iter\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3- Tester et comparer les deux solutions \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédiction des probabilités avec les trois modèles binaires\n",
    "#modèle 1\n",
    "iris_setosa_prob = reg_iris_setosa.predire(X_iris_test)\n",
    "#modèle 2\n",
    "iris_versicolor_prob = reg_iris_versicolor.predire(X_iris_test)\n",
    "#modèle 3\n",
    "iris_virginica_prob = reg_iris_virginica.predire(X_iris_test)\n",
    "\n",
    "# fusionner les probabilités en une matrice\n",
    "iris_modeles3_prob = np.column_stack((iris_setosa_prob, iris_versicolor_prob, iris_virginica_prob))\n",
    "\n",
    "# encodage des sorties en 1, 2, 3\n",
    "iris_modeles3_test = np.argmax(iris_modeles3_prob, axis=1) + 1\n",
    "\n",
    "iris_modeles3_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédiction des probabilités avec le modèle multinomial\n",
    "iris_maxent_prob = maxent_iris.predire(X_iris_test)\n",
    "\n",
    "# encodage des sorties en 1, 2, 3\n",
    "iris_maxent_test = np.argmax(iris_maxent_prob, axis=1) + 1\n",
    "\n",
    "iris_maxent_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédiction des probabilités avec le modèle multinomial avec 5 itérations\n",
    "iris_maxent20_prob = maxent20_iris.predire(X_iris_test)\n",
    "\n",
    "# encodage des sorties en 1, 2, 3\n",
    "iris_maxent20_test = np.argmax(iris_maxent20_prob, axis=1) + 1\n",
    "\n",
    "iris_maxent20_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "noms_classes = [\"setosa\", \"versicolor\", \"virginica\"]\n",
    "\n",
    "# Par 3 modèles\n",
    "print(\"En utilisant 3 modèles de régression binaire\")\n",
    "print(classification_report(Y_iris_test, iris_modeles3_test, target_names=noms_classes))\n",
    "\n",
    "# Par 1 modèle multinomial\n",
    "print(\"En utilisant 1 modèle de régression multinomiale\")\n",
    "print(classification_report(Y_iris_test, iris_maxent_test, target_names=noms_classes))\n",
    "\n",
    "# Par 1 modèle multinomial (20 itérations)\n",
    "print(\"En utilisant 1 modèle de régression multinomiale (20 itérations)\")\n",
    "print(classification_report(Y_iris_test, iris_maxent20_test, target_names=noms_classes))\n",
    "\n",
    "# Ignorer le Warning en bas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** : Anlysez les résulats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
